好的，这是为您的《大前端与AI实战》实训课程设计的第七次课程的详细内容。本次课程将引入多媒体和AI能力，是项目从一个“内容管理系统”向“智能学习平台”转变的关键一步。

---

### **《大前端与AI实战》第七次课程：费曼学习法核心 - 录音与语音转文字**

**课程主题：** “说”出你的理解：Web录音与AI语音识别
**总时长：** 4学时 (约3-3.5小时教学，半小时答疑与休息)

#### **一、 本次课程目标 (Objectives)**

在本次课程结束后，每位同学都应该能够：
1.  **理解** 浏览器 `MediaStream` 和 `MediaRecorder` API 的工作原理。
2.  **在React组件中实现** 网页录音功能，包括开始、暂停、停止录音，并能本地回放。
3.  **将录制的音频**（Blob对象）转换为文件（File对象）并通过 `FormData` 上传到后端。
4.  **在Node.js后端** 配置 `multer` 中间件来接收和处理上传的音频文件。
5.  **注册并获取** 百度智能云的API Key和Secret Key，了解其控制台基本操作。
6.  **在后端调用百度语音识别API**，将上传的音频文件转换为文字，并返回给前端。
7.  **将转录后的文字** 显示在前端页面上，完成“说->存->转->显”的核心流程。

#### **二、 核心关键词 (Keywords)**

*   `MediaStream` API
*   `MediaRecorder` API
*   `Blob` (二进制大对象)
*   `FormData`
*   `multer` (Node.js文件上传中间件)
*   百度智能云 (Baidu AI Cloud)
*   语音识别 (ASR - Automatic Speech Recognition)
*   REST API 调用 (后端到第三方)

---

### **三、 详细教学流程 (Step-by-Step Guide)**

---

#### **第一部分：前端实现网页录音 (约75分钟)**

**教师讲解与带领编码：**

1.  **回顾与承接**
    *   “上节课，我们实现了知识点的内容管理和富文本渲染。现在，我们的平台已经是一个优秀的笔记软件了。但我们的目标是‘费曼学习平台’，核心在于‘讲解’和‘反馈’。今天，我们就来实现‘讲解’的第一步——让用户能通过麦克风录下他们对知识点的复述。”

2.  **Web 录音API讲解**
    *   **`navigator.mediaDevices.getUserMedia()`:**
        *   **讲解：** “这是浏览器提供的魔法棒，可以向用户请求使用他们的摄像头和/或麦克风。它会返回一个 `MediaStream` 对象，就像一条包含了实时音视频数据流的‘管道’。”
        *   **安全注意：** 这个API必须在 `HTTPS` 或 `localhost` 环境下才能使用，且必须得到用户的明确授权。
    *   **`MediaRecorder`:**
        *   **讲解：** “`MediaRecorder` 就像一个录像机。我们把 `MediaStream` 这条‘管道’接在它上面，它就能把流经的数据录制下来，打包成一个文件（`Blob`对象）。”
        *   **核心事件：**
            *   `onstart`: 开始录制时触发。
            *   `ondataavailable`: 当一小块数据准备好时触发。我们需要收集这些数据块。
            *   `onstop`: 停止录制时触发。这时我们可以把所有收集到的数据块整合成一个完整的音频文件。

3.  **创建录音组件**
    *   **思路：** 我们可以创建一个专门的页面或在一个模态框(Modal)中进行录音。为了简化，我们先创建一个新的页面 `FeynmanRecordPage.jsx`。
    *   在 `src/pages` 下新建 `FeynmanRecordPage.jsx`。
    *   **安装一个简单的录音hook库（可选，但推荐以简化教学）**：`react-media-recorder`。
        ```bash
        npm install react-media-recorder
        ```
        *   **教师说明：** “我们可以自己从零开始写 `MediaRecorder` 的逻辑，但涉及很多状态管理会比较繁琐。使用一个成熟的Hook库可以让我们更专注于业务流程。课后大家可以去研究它的源码，了解底层实现。”
    *   **编写 `FeynmanRecordPage.jsx`:**
        ```jsx
        // src/pages/FeynmanRecordPage.jsx
        import { useReactMediaRecorder } from 'react-media-recorder';
        import { useParams } from 'react-router-dom';
        import { useState, useEffect } from 'react';
        import apiClient from '../api/axios';

        function FeynmanRecordPage() {
            const { id } = useParams(); // 知识点ID
            const [kpTitle, setKpTitle] = useState('');
            const [transcribedText, setTranscribedText] = useState('');
            const [isUploading, setIsUploading] = useState(false);

            // 使用Hook
            const { status, startRecording, stopRecording, mediaBlobUrl } = useReactMediaRecorder({ audio: true });

            useEffect(() => {
                // 获取知识点标题用于显示
                const fetchKpTitle = async () => {
                    const response = await apiClient.get(`/knowledge-points/${id}`);
                    setKpTitle(response.data.title);
                };
                fetchKpTitle();
            }, [id]);

            const handleStopRecording = async () => {
                stopRecording(); // 这个库的stopRecording是异步的，但我们可以在onStop回调中处理
            };

            const uploadAudio = async (blobUrl) => {
                setIsUploading(true);
                setTranscribedText('');
                try {
                    const audioBlob = await fetch(blobUrl).then(r => r.blob());
                    const audioFile = new File([audioBlob], `feynman-record-${id}.wav`, { type: 'audio/wav' });

                    const formData = new FormData();
                    formData.append('audio', audioFile); // 'audio'要和后端multer的字段名一致
                    formData.append('knowledgePointId', id); // 顺便把知识点ID也传过去

                    const response = await apiClient.post('/audio/transcribe', formData, {
                        headers: {
                            'Content-Type': 'multipart/form-data',
                        },
                    });
                    
                    setTranscribedText(response.data.result);
                } catch (error) {
                    console.error('上传或转录失败', error);
                    setTranscribedText('转录失败，请重试。');
                } finally {
                    setIsUploading(false);
                }
            };
            
            // 改造 useReactMediaRecorder，使其在停止时自动上传
            const { status: recStatus, startRecording: recStart, stopRecording: recStop, mediaBlobUrl: recUrl } = useReactMediaRecorder({ 
              audio: true,
              onStop: (blobUrl, blob) => {
                uploadAudio(blobUrl);
              }
            });


            return (
                <div>
                    <h1>复述知识点: {kpTitle}</h1>
                    <p>录音状态: {recStatus}</p>
                    
                    <button onClick={recStart} disabled={recStatus === 'recording'}>开始录音</button>
                    <button onClick={recStop} disabled={recStatus !== 'recording'}>停止录音</button>

                    {recUrl && <audio src={recUrl} controls />}

                    <hr />

                    <h2>AI 转录结果:</h2>
                    {isUploading && <p>正在上传并转录，请稍候...</p>}
                    <div style={{ border: '1px solid #ccc', padding: '1rem', minHeight: '100px' }}>
                        {transcribedText}
                    </div>
                </div>
            );
        }

        export default FeynmanRecordPage;
        ```
    *   **添加路由和入口：**
        *   在 `App.jsx` 中添加路由：`<Route path="/feynman/:id" element={<FeynmanRecordPage />} />`
        *   在 `DashboardPage.jsx` 的知识点列表项中，添加一个“开始复述”的链接或按钮：`<Link to={`/feynman/${kp._id}`}>开始复述</Link>`。

---

#### **第二部分：后端接收音频文件 (约45分钟)**

**教师讲解与带领后端编码：**

1.  **安装 `multer`**
    *   **打开后端项目** (`feynman-platform-backend`)。
    *   在终端中安装：
        ```bash
        npm install multer
        ```
    *   **讲解：** “Express 默认不处理 `multipart/form-data` 这种格式的请求体，这种格式通常用于文件上传。`multer` 就是一个专门处理这种请求的中间件，它会解析上传的文件，并把文件信息放到 `req.file` 或 `req.files` 对象上。”

2.  **配置 `multer` 和创建上传路由**
    *   在后端 `routes` 文件夹下新建 `audio.js` 文件。
    ```javascript
    // routes/audio.js
    const express = require('express');
    const router = express.Router();
    const multer = require('multer');
    const auth = require('../middleware/auth');
    // 引入百度AI的控制器（我们稍后创建）
    const { transcribeAudio } = require('../controllers/baiduAiController');

    // 配置multer
    // 我们这里使用内存存储，因为只是临时中转给百度AI，不需要存到服务器硬盘
    const upload = multer({ storage: multer.memoryStorage() });

    // @route   POST /api/audio/transcribe
    // @desc    上传音频并进行语音识别
    // @access  Private
    router.post(
        '/transcribe',
        auth,
        upload.single('audio'), // 'audio' 必须和前端 FormData.append 的字段名一致
        transcribeAudio // 将主要逻辑放到Controller中
    );

    module.exports = router;
    ```
    *   **在 `index.js` 中使用该路由：**
        ```javascript
        // backend/index.js
        // ...
        app.use('/api/audio', require('./routes/audio'));
        ```

---

#### **第三部分：集成百度AI语音识别 (约75分钟)**

**教师讲解与带领编码：**

1.  **注册百度智能云并创建应用**
    *   **操作指引 (一步步带领学生):**
        1.  访问 [百度智能云官网](https://cloud.baidu.com/) 并注册/登录账号，完成实名认证（通常学生认证即可）。
        2.  进入控制台，找到“人工智能” -> “语音技术”。
        3.  进入语音技术控制台，点击“创建应用”，填写应用名称（如“费曼学习平台”），选择“语音识别”服务，应用描述随便写。
        4.  创建成功后，在应用列表中可以看到刚刚创建的应用。记下它的 `AppID`, `API Key` 和 `Secret Key`。**这是非常重要的凭证！**

2.  **将凭证存入后端 `.env` 文件**
    ```.env
    # ... other keys
    BAIDU_APP_ID=your_app_id
    BAIDU_API_KEY=your_api_key
    BAIDU_SECRET_KEY=your_secret_key
    ```
    **强调：** 再次提醒学生 `.env` 文件不能提交到 Git。

3.  **安装百度 AI SDK**
    *   在后端项目终端中，安装官方提供的 SDK：
        ```bash
        npm install baidu-aip-sdk
        ```

4.  **创建百度AI控制器**
    *   在后端项目根目录新建 `controllers` 文件夹。
    *   在 `controllers` 中新建 `baiduAiController.js`。
    ```javascript
    // controllers/baiduAiController.js
    const AipSpeechClient = require("baidu-aip-sdk").speech;

    // 从环境变量中获取凭证
    const APP_ID = process.env.BAIDU_APP_ID;
    const API_KEY = process.env.BAIDU_API_KEY;
    const SECRET_KEY = process.env.BAIDU_SECRET_KEY;

    // 新建一个AipSpeechClient对象
    const client = new AipSpeechClient(APP_ID, API_KEY, SECRET_KEY);

    exports.transcribeAudio = async (req, res) => {
        if (!req.file) {
            return res.status(400).json({ msg: 'No audio file uploaded.' });
        }

        // req.file.buffer 包含了音频文件的二进制数据
        const audioBuffer = req.file.buffer;

        try {
            // 调用语音识别短语音版
            // 'wav' 是文件格式, 16000 是采样率, dev_pid: 1537 是普通话模型
            const result = await client.recognize(audioBuffer, 'wav', 16000, {
                dev_pid: 1537,
            });

            console.log('Baidu ASR Result:', result);

            // 检查返回结果
            if (result.err_no === 0) {
                // 成功
                res.json({ result: result.result[0] });
            } else {
                // 失败
                res.status(500).json({ msg: 'Baidu ASR service error', error: result });
            }

        } catch (error) {
            console.error('Error calling Baidu ASR API:', error);
            res.status(500).send('Server error during transcription.');
        }
    };
    ```

---

#### **第四部分：联调测试与总结 (15分钟)**

**教师引导学生进行完整流程测试：**

1.  **启动后端服务**，确保所有新代码和配置已加载。
2.  **启动前端服务**。
3.  登录账号，进入 Dashboard。
4.  点击任意知识点的“开始复述”按钮，进入录音页面。
5.  浏览器会弹出请求麦克风权限的提示，**点击“允许”**。
6.  点击“开始录音”，对着麦克风说一段关于这个知识点的话（比如：“React是一个用于构建用户界面的JavaScript库。”）。
7.  点击“停止录音”。
8.  观察前端页面，应该会显示“正在上传并转录...”，然后显示出转录后的文字。
9.  同时观察后端服务器的控制台，应该会打印出从百度AI返回的原始结果。

---

#### **五、 课堂总结与作业 (15分钟)**

*   **总结：**
    *   “今天我们实现了‘费曼学习平台’中最具互动性和技术挑战性的功能之一！我们学会了如何从前端捕获用户的声音，通过后端将其上传，并调用强大的第三方AI服务将语音转换成文字。这不仅仅是技术的堆砌，而是我们第一次真正地将 AI 能力融入到我们的产品流程中，让机器能够‘听懂’用户的语言。这是迈向智能化的一大步！”
*   **课后作业：**
    1.  **必须完成：** 确保整个“录音->上传->转录->显示”流程能够顺畅运行。
    2.  **功能扩展（选做）：**
        *   在 `FeynmanRecordPage.jsx` 中，将转录后的文字保存下来。可以添加一个“保存复述”按钮，点击后调用一个新的后端API，将 `knowledgePointId` 和 `transcribedText` 存到数据库中（可能需要新建一个`FeynmanAttempt`之类的数据模型）。
        *   优化UI，比如在录音时显示一个动态的计时器或者音量波形图（这个比较有挑战性）。
*   **预告下次课内容：**
    *   “我们已经能把用户的复述转成文字了，下一步该做什么？当然是让AI来评价这份‘答卷’！下次课，我们将把转录后的文字，连同原始知识点内容一起，发送给**文心大模型**，设计巧妙的 Prompt，让 AI 对学生的复述进行**文本润色、评价和打分**，完成费曼学习法的核心闭环！”

**答疑环节，课程结束。**